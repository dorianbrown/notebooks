{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-Words for Document Classification\n",
    "\n",
    "## Data and problem statement\n",
    "\n",
    "We consider a situation where a list of English language documents are given and a class to which each of these belongs. We would like to extract a set of features from each document (in a consistent manner) and to choose a classification model to train on these data. As always, we need to split our data into train, validation and test partition or apply k-fold cross validation. In this training we will simplify by simply splitting this documents list at random into train and test parts (0.70 train and 0.30 test).\n",
    "\n",
    "> For this training you'll need the *uci-news-aggregator.csv* dataset from [this kaggle project](https://www.kaggle.com/uciml/news-aggregator-dataset). You'll need to download it if you don't have it already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP']\n",
      "Dimensions: (422419, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CATEGORY\n",
       "b    115967\n",
       "e    152469\n",
       "m     45639\n",
       "t    108344\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "text_df = pd.read_csv(\"~/workspace/data/uci-news-aggregator.csv\")\n",
    "\n",
    "# Some properties of our data\n",
    "print(\"Columns:\", text_df.columns.tolist())\n",
    "print(\"Dimensions:\", text_df.shape)\n",
    "\n",
    "text_df.groupby(\"CATEGORY\").size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will limit our document dataset to two classes (b = business, e = entertainment) so that we have a binary classification problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CATEGORY\n",
       "b    115967\n",
       "e    152469\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df = text_df.query(\"CATEGORY == 'b' | CATEGORY == 'e'\")\n",
    "text_df.groupby(\"CATEGORY\").size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic text preprocessing with NLTK\n",
    "\n",
    "Since language is simply too rich to define separate variable for each word form that appears in or documents, we first transform our texts as follows, and thereby reduce number of distinct words.\n",
    "\n",
    "1. Remove all punctuation signs and digits\n",
    "2. Cast all letters to lower case\n",
    "3. Remove words that appear very often (thus are little informative); we use a file from a package where standard list of stop words is found\n",
    "4. Apply a lematization algorithm; there are many variants and essentially we wish to map e.g. ‘be’, ‘being’, ‘am’ , ‘is’ to a single word ‘be’; likewise ‘element’, ‘elements’, ‘elementary’, ‘elemental’ should be mapped to ‘element’. However ‘news’ should not map to ‘new’ as it is a distinct word. There are many scientific articles about how to do this in English language, and off course less for other languages and the proposed solutions also vary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fed official says weak data caused by weather, should not slow taper\n",
      "fed offici say weak data caus weather slow taper\n",
      "---\n",
      "Fed's Charles Plosser sees high bar for change in pace of tapering\n",
      "fed charl plosser see high bar chang pace taper\n",
      "---\n",
      "US open: Stocks fall after Fed official hints at accelerated tapering\n",
      "open stock fall fed offici hint acceler taper\n",
      "---\n",
      "Fed risks falling 'behind the curve', Charles Plosser says\n",
      "fed risk fall curv charl plosser say\n",
      "---\n",
      "Fed's Plosser: Nasty Weather Has Curbed Job Growth\n",
      "fed plosser nasti weather curb job growth\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Run first time to download corpora\n",
    "# import nltk\n",
    "# nltk.download(\"wordnet\")\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "wnl = WordNetLemmatizer()\n",
    "prt = PorterStemmer()\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove punctuation and digits\n",
    "    text = re.sub(r'[^A-Za-z\\s]','',text)  # Only keep words and whitespace characters\n",
    "    # Make lower case\n",
    "    text = text.lower()\n",
    "    # Split into list for further transformations\n",
    "    text = text.split(\" \")\n",
    "    # Remove stopwords\n",
    "    text = [word for word in text if word not in STOPWORDS]\n",
    "    # Lemmatize and stemming words\n",
    "    text = [prt.stem(word) for word in text]\n",
    "    text = [wnl.lemmatize(word) for word in text]\n",
    "    # Return to string\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "for title in text_df.TITLE[0:5]:\n",
    "    print(title)\n",
    "    print(clean_text(title))\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating binary features from documents\n",
    "\n",
    "A human typed text is a rich representation of information which essentially requires human level of intelligence to fully understand. In particular order of words, punctuation signs, and exact grammatical forms in which words are used gives many different flavours to a piece of text. However we are only interested to separate a list of documents in two piles-one of class 0 and the other of class 1.\n",
    "\n",
    "**Simplest approach one can take is to define one binary variable for each word that appears in any of the documents that we have to work with in this problem, in such way that if that word appears in a given document, then this feature has value 1 and of it does not appear then it has value 0.**\n",
    "\n",
    "In this approach, if a word appears more than once, then the corresponding feature still has value 1. Also, the order of the words does not influence the extracted feature values, and also punctuation signs are disregarded all together (in fact we will remove them at the beginning of our processing).\n",
    "\n",
    "As example ‘Story was telling about a data scientist’ and ‘Data scientist was telling a story about data’ will have exactly the same feature values sequence. As inadequate as this may seem, if the goal is to find documents about data scientists then it might in fact work well enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa', 'aaa', 'aaaa', 'aaaarrrrrghhh', 'aaah', 'aal', 'aaliyah', 'aamc', 'aan', 'aap']\n",
      "34402\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=\"word\", \n",
    "                             binary=True,\n",
    "                             preprocessor=clean_text)\n",
    "\n",
    "tdm = vectorizer.fit_transform(text_df.TITLE)\n",
    "\n",
    "print(vectorizer.get_feature_names()[0:10])\n",
    "print(len(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier with binary features\n",
    "\n",
    "One of the most simple types of machine learning models that is suitable here is the so called Naïve Bayes model. The simplest form of this model is of the binary classification type. There we assume that each observation consists of a vector of binary valued features $X=(x_1,x_2,…,x_k)$ (each component $x_j$ is either 0 or 1 valued) and a binary target value $Y$. By the basic Bayes theorem we have that\n",
    "\n",
    "\\begin{align*} P(Y \\mid x_1,x_2,...,x_k) &= \\frac{P(Y)\\ P(x_1,x_2,...,x_k \\mid Y)}{P(x_1,x_2,...,x_k)} \\\\\n",
    "&= \\frac{P(Y)\\ P(x_1,x_2,...,x_k \\mid Y)}{P(x_1,x_2,...,x_k\\mid Y = 0)\\ P(Y=0) + P(x_1,x_2,...,x_k\\mid Y = 1)\\ P(Y=1)}\\end{align*}\n",
    "\n",
    "Here $X=(x_1,x_2,…,x_k)$ is a binary valued sequence of feature values for a given observation. In this case we 'only' need to estimate probabilities $P(Y=1)$, $P(Y=0)$, and $P(x_1,x_2,…,x_k|Y=0)$, $P(x_1,x_2,…,x_k|Y=1)$ for each possible binary sequence $x_1,x_2,...,x_k$ of feature values. Now for a feature vector of length $k$, there are $2^k$ distinct binary sequences of length $k$ (in each position we can have 0 or 1, so we indeed have to take k-th power of 2). So total number of parameters to estimate is $2\\cdot2^k +2$ = 'waaay too many!'.\n",
    "\n",
    "To get a feeling of this number, if we have many features, say as many as there are different words in a list of documents, of we would work with 200 different words, then $2^{200}=16^{50}$ which is a number larger then number of atoms in the universe! Also, Some of these combinations of binary values are typically very rare in our data which amounts to very low significance (certainty) of our estimate, and that translates directly to low predictive performance of our model.\n",
    "\n",
    "## Reducing number of parameters\n",
    "\n",
    "A way to mitigate a too high number of parameters to estimate is the so called Naïve Bayes assumption.  We thus assume that\n",
    "\n",
    "$$ P(x_1,x_2,...,x_k \\mid Y=0) = \\prod_{i=1}^{k}P(x_i \\mid Y=0)$$\n",
    "\n",
    "and\n",
    "\n",
    "$$ P(x_1,x_2,...,x_k \\mid Y=1) = \\prod_{i=1}^{k}P(x_i \\mid Y=1)$$\n",
    "\n",
    "for each sequence of binary values $x_1, x_2, …, x_k$.\n",
    "\n",
    "This greatly reduces the number of probabilities we need to estimate, now only $k$ for $p(x_1,x_2,...,x_k)$. This means a total of $2\\cdot k + 2$, for a total speedup of $\\frac{2\\cdot k + 2}{2 \\cdot 2^k + 2}$. This speedup as a function of $k$ is plotted below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9507154630>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.arange(1,10)\n",
    "plt.plot(x, (2*2**x + 2)/(2*x + 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can estimate these probabilities using a maximum likelihood estimator, which gives us the following estimators:\n",
    "\n",
    "$$p^i_{1,0} = P(x_i=1\\mid Y=0) := \\frac{\\#\\{\\text{observations with $x_i = 1$ and $y = 0$}\\} + 1}{\\#\\{\\text{observations with $Y=0$}\\} + 2}$$\n",
    "\n",
    "$$p^i_{0,0} = P(x_i=0\\mid Y=0) := \\frac{\\#\\{\\text{observations with $x_i = 0$ and $y = 0$}\\} + 1}{\\#\\{\\text{observations with $Y=0$}\\} + 2}$$\n",
    "\n",
    "$$ p^i_{0,1} = P(x_i=0\\mid Y=1) := \\frac{\\#\\{\\text{observations with $x_i = 0$ and $y = 1$}\\} + 1}{\\#\\{\\text{observations with $Y=1$}\\} + 2}.$$\n",
    "\n",
    "$$ p^i_{1,1} = P(x_i=1\\mid Y=1) := \\frac{\\#\\{\\text{observations with $x_i = 1$ and $y = 1$}\\} + 1}{\\#\\{\\text{observations with $Y=1$}\\} + 2}.$$\n",
    "\n",
    "and\n",
    "\n",
    "$$ q^0 = P(Y=0) := \\frac{\\#\\{\\text{observations with $Y = 0$}\\} + 1}{\\#\\{\\text{total observations}\\} + 2}.$$\n",
    "\n",
    "$$ q^1 = P(Y=1) := \\frac{\\#\\{\\text{observations with $Y = 1$}\\} + 1}{\\#\\{\\text{total observations}\\} + 2}.$$\n",
    "\n",
    "Since this is binary classification, we could also calculate some of these probabilites by using $p^i_{0,0} = 1 - p^i_{1,0}$. We show the complete calculations above as this more easily generalizes to non-binary classification.\n",
    "\n",
    "> *Note that we add 1 to the numerator and 2 to the denominator. This is what's called Laplace smoothing, which improves the estimator's accuracy and makes it more robust. You can read more about it [here](https://www.wikiwand.com/en/Additive_smoothing).*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although above we show how each individual $p^i_{1,0}$ is calculated, it is easier to calculate the vector \n",
    "\n",
    "$$p_{1,0} = \\big(p^1_{1,0}, p^2_{1,0}, ..., p^k_{1,0}\\big)$$\n",
    "\n",
    "where $p^i_{1,0} = P(x_i = 1 \\mid Y=0)$. The same counts for the other probabilities $p^i_{0,1}, q^0,$ etc...\n",
    "\n",
    "## How to classify a new documents\n",
    "\n",
    "Say we have a new out-of-sample title we want to classify. The feature vector associated with sentence (after all the necessary cleaning steps) is $X = (x_1,x_2,...,x_k)$, where $x_i = 1$ if the $i$-th word is in the new sentence, and $x_i = 0$ if not. We can then calculate the new probability using:\n",
    "\n",
    "$$ P(Y = 1 \\mid x_1,...,x_k) = \\frac{q^1\\cdot \\left(\\prod_{i=1}^{k}p^i_{x_i,1}\\right)}{q^0\\cdot \\left(\\prod_{i=1}^{k}p^i_{x_i,0}\\right) + q^1\\cdot \\left(\\prod_{i=1}^{k}p^i_{x_i,1}\\right)}$$\n",
    "\n",
    "Now to calculate this probability, we will use an example sentence valued `title = \"President trump actually launched a nuclear missile at the moon. We're doomed!\"`. We will use the shorter vector notation $p_{0,0}$ and $q^0$ mentioned earlier, as this works nicely with numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  229  8262 16864 19331 19640 21088 23726 31171]\n",
      "['actual', 'doom', 'launch', 'missil', 'moon', 'nuclear', 'presid', 'trump']\n"
     ]
    }
   ],
   "source": [
    "target = text_df.CATEGORY == \"b\"\n",
    "\n",
    "p10 = ((tdm == 1).sum(axis=0) + 1)/(sum(target == False) + 2)\n",
    "p00 = 1 - p10\n",
    "p01 = ((tdm == 1).sum(axis=0) + 1)/(sum(target == True) + 2)\n",
    "p11 = 1 - p01\n",
    "q0 = (sum(target == False) + 1)/len(target)\n",
    "q1 = 1 - q0\n",
    "\n",
    "# We'll convert our sentence to it's binary feature vector first\n",
    "title = \"President trump actually launched a nuclear missile at the moon. We're doomed!\"\n",
    "output = vectorizer.transform([title])\n",
    "print(output.indices)\n",
    "print([vectorizer.get_feature_names()[i] for i in output.indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our text was properly preprocessed, and we now have our sentence's feature vector. Lets get to calculated it's probability. We'll first calculate the $q^1\\cdot \\left(\\prod_{i=1}^{k}p^i_{x_i,1}\\right)$ term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# These are the probability terms for x_i = 1 and x_i = 0\n",
    "xi1_probs = p11.transpose()[output.indices]\n",
    "xi0_probs = np.delete(p01,output.indices)\n",
    "numerator = q1*np.prod(xi1_probs)*np.prod(xi0_probs)\n",
    "print(numerator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh-oh, this looks like a [numerical underflow](https://www.wikiwand.com/en/Arithmetic_underflow) issue. This happens when a number gets so small that the precision at which python stores a number becomes insufficient. The number is so small that python considers it to be zero. Lets test that theory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerator == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep. Luckily some creative minds came up with as solution to this problem. Have a look at [this stackexchange post](https://stats.stackexchange.com/questions/105602/example-of-how-the-log-sum-exp-trick-works-in-naive-bayes) for a great explanation. We instead calculate the following, which avoids the issues of numerical underflow using the magic of logarithms:\n",
    "\n",
    "$$\\log\\big(P(Y=1\\mid x_1,...,x_k)\\big) = \\log\\left(q^0\\right) + \\sum_{i=1}^k\\log\\left(p_{x_i,1}^i\\right) - M$$\n",
    "\n",
    "with\n",
    "\n",
    "$$ M = A + \\log\\left(e^{\\log\\left(q_0\\right) + \\sum_{i=1}^k\\log\\left(p^i_{x_i,0}\\right) - A} + e^{\\log\\left(q_1\\right) + \\sum_{i=1}^k\\log\\left(p^i_{x_i,1}\\right) - A}\\right)$$\n",
    "\n",
    "where we take $A:= \\max\\left(\\log\\left(q_0\\right) + \\sum_{i=1}^k\\log\\left(p^i_{x_i,0}\\right), \\log\\left(q_1\\right) + \\sum_{i=1}^k\\log\\left(p^i_{x_i,1}\\right)\\right)$. Let's try this out and see if we are able to avoid the underflow issue we had before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-67.36642227626015 -336327.3844962771\n",
      "-67.36642227626015\n"
     ]
    }
   ],
   "source": [
    "# We use logpxy1 = log( P(x_1,...,x_k | Y = 1) )\n",
    "logpxy1 = np.log(q1) + np.sum(np.log(p11.transpose()[output.indices])) + np.sum(np.log(np.delete(p01,output.indices)))\n",
    "logpxy0 = np.log(q0) + np.sum(np.log(p10.transpose()[output.indices])) + np.sum(np.log(np.delete(p00,output.indices)))\n",
    "print(logpxy0, logpxy1)\n",
    "A = max(logpxy0,logpxy1)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term `np.exp(logpxy1 - A)` still overflows, so we're just going to predict the class directly from `max(logpxy0,logpxy1)` which is also optimal.\n",
    "\n",
    "Combing this all together into a function gives us the following class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifier:\n",
    "\n",
    "    def clean_text(text):\n",
    "        # Remove punctuation and digits\n",
    "        text = re.sub(r'[^A-Za-z\\s]','',text)  # Only keep words and whitespace characters\n",
    "        # Make lower case\n",
    "        text = text.lower()\n",
    "        # Split into list for further transformations\n",
    "        text = text.split(\" \")\n",
    "        # Remove stopwords\n",
    "        text = [word for word in text if word not in STOPWORDS]\n",
    "        # Lemmatize and stemming words\n",
    "        text = [prt.stem(word) for word in text]\n",
    "        text = [wnl.lemmatize(word) for word in text]\n",
    "        # Return to string\n",
    "        text = \" \".join(text)\n",
    "        return text\n",
    "\n",
    "    \n",
    "    def fit(self, doc_list, target):\n",
    "        \n",
    "        self.vectorizer = CountVectorizer(analyzer=\"word\", \n",
    "                                          binary=True,\n",
    "                                          preprocessor=clean_text)\n",
    "        \n",
    "        tdm = self.vectorizer.fit_transform(doc_list)\n",
    "        \n",
    "        self.p10 = ((tdm == 1).sum(axis=0) + 1)/(sum(target == False) + 2)\n",
    "        self.p00 = 1 - self.p10\n",
    "        self.p11 = ((tdm == 1).sum(axis=0) + 1)/(sum(target == True) + 2)\n",
    "        self.p01 = 1 - self.p11\n",
    "        self.q0 = (sum(target == False) + 1)/len(target)\n",
    "        self.q1 = 1 - self.q0\n",
    "        \n",
    "    def predict(self, sentences):\n",
    "        bin_feat = self.vectorizer.transform(sentences).transpose()\n",
    "        n = len(sentences)\n",
    "        \n",
    "        p00 = self.p00\n",
    "        p01 = self.p01\n",
    "        p10 = self.p10\n",
    "        p11 = self.p11\n",
    "        q1 = self.q1\n",
    "        q0 = self.q0\n",
    "        \n",
    "        logpxy1 = np.ones(n)*np.log(q1) + np.log(p11*bin_feat + \n",
    "                                                 p01*np.ones(bin_feat.shape) - p01*bin_feat)\n",
    "        logpxy0 = np.ones(n)*np.log(q0) + np.log(p10*bin_feat + \n",
    "                                                 p00*np.ones(bin_feat.shape) - p00*bin_feat)\n",
    "        \n",
    "        return logpxy1 > logpxy0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target = text_df.CATEGORY == \"b\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_df.TITLE[0:10000], target[0:10000], test_size=0.3)\n",
    "\n",
    "binclass = BinaryClassifier()\n",
    "binclass.fit(X_train,y_train)\n",
    "y_test_pred = binclass.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3000)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
