{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-Words for Document Classification\n",
    "\n",
    "## Data and problem statement\n",
    "\n",
    "We consider a situation where a list of English language documents are given and a class to which each of these belongs. We would like to extract a set of features from each document (in a consistent manner) and to choose a classification model to train on these data. As always, we need to split our data into train, validation and test partition or apply k-fold cross validation. In this training we will simplify by simply splitting this documents list at random into train and test parts (0.70 train and 0.30 test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP']\n",
      "Dimensions: (422419, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CATEGORY\n",
       "b    115967\n",
       "e    152469\n",
       "m     45639\n",
       "t    108344\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df = pd.read_csv(\"/home/dorian/workspace/data/uci-news-aggregator.csv\")\n",
    "\n",
    "# Some properties of our data\n",
    "print(\"Columns:\", text_df.columns.tolist())\n",
    "print(\"Dimensions:\", text_df.shape)\n",
    "\n",
    "text_df.groupby(\"CATEGORY\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CATEGORY\n",
       "b    115967\n",
       "e    152469\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df = text_df.query(\"CATEGORY == 'b' | CATEGORY == 'e'\")\n",
    "text_df.groupby(\"CATEGORY\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = text_df.sample(frac=0.7)\n",
    "test_text = text_df.drop(train_text.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic text preprocessing with NLTK\n",
    "\n",
    "Since language is simply too rich to define separate variable for each word form that appears in or documents, we first transform our texts as follows, and thereby reduce number of distinct words.\n",
    "\n",
    "1. Remove all punctuation signs and digits\n",
    "2. Cast all letters to lower case\n",
    "3. Remove words that appear very often (thus are little informative); we use a file from a package where standard list of stop words is found\n",
    "4. Apply a lematization algorithm; there are many variants and essentially we wish to map e.g. ‘be’, ‘being’, ‘am’ , ‘is’ to a single word ‘be’; likewise ‘element’, ‘elements’, ‘elementary’, ‘elemental’ should be mapped to ‘element’. However ‘news’ should not map to ‘new’ as it is a distinct word. There are many scientific articles about how to do this in English language, and off course less for other languages and the proposed solutions also vary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fed official says weak data caused by weather, should not slow taper\n",
      "['fed' 'official' 'say' 'weak' 'data' 'caused' 'weather' 'slow' 'taper']\n",
      "---\n",
      "Fed's Charles Plosser sees high bar for change in pace of tapering\n",
      "['fed' 'charles' 'plosser' 'see' 'high' 'bar' 'change' 'pace' 'tapering']\n",
      "---\n",
      "US open: Stocks fall after Fed official hints at accelerated tapering\n",
      "['u' 'open' 'stock' 'fall' 'fed' 'official' 'hint' 'accelerated'\n",
      " 'tapering']\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "# Some setup (nltk weirdness)\n",
    "lemma = WordNetLemmatizer()\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove punctuation and digits\n",
    "    text = re.sub(r'[^\\w\\s]','',text)  # Only keep words and whitespace characters\n",
    "    # Make lower case\n",
    "    text = text.lower()\n",
    "    # Split into list for further transformations\n",
    "    text = text.split(\" \")\n",
    "    # Remove stopwords\n",
    "    text = [word for word in text if word not in stopWords]\n",
    "    # Lemmatize and stemming words\n",
    "    text = np.array([lemma.lemmatize(word) for word in text])\n",
    "    return text\n",
    "\n",
    "for title in text_df.TITLE[0:3]:\n",
    "    print(title)\n",
    "    print(clean_text(title))\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268436\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[array(['fed', 'official', 'say', 'weak', 'data', 'caused', 'weather',\n",
       "         'slow', 'taper'], dtype='<U8'), True],\n",
       " [array(['fed', 'charles', 'plosser', 'see', 'high', 'bar', 'change',\n",
       "         'pace', 'tapering'], dtype='<U8'), True],\n",
       " [array(['u', 'open', 'stock', 'fall', 'fed', 'official', 'hint',\n",
       "         'accelerated', 'tapering'], dtype='<U11'), True]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_list = [[clean_text(x[1].TITLE), x[1].CATEGORY == 'b']\n",
    "                for x in text_df.iterrows()]\n",
    "print(len(bow_list))\n",
    "bow_list[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating binary features from documents\n",
    "\n",
    "A human typed text is a rich representation of information which essentially requires human level of intelligence to fully understand. In particular order of words, punctuation signs, and exact grammatical forms in which words are used gives many different flavours to a piece of text. However we are only interested to separate a list of documents in two piles-one of class 0 and the other of class 1.\n",
    "\n",
    "**Simplest approach one can take is to define one binary variable for each word that appears in any of the documents that we have to work with in this problem, in such way that if that word appears in a given document, then this feature has value 1 and of it does not appear then it has value 0.**\n",
    "\n",
    "In this approach, if a word appears more than once, then the corresponding feature still has value 1. Also, the order of the words does not influence the extracted feature values, and also punctuation signs are disregarded all together (in fact we will remove them at the beginning of our processing).\n",
    "\n",
    "As example ‘Story was telling about a data scientist’ and ‘Data scientist was telling a story about data’ will have exactly the same feature values sequence. As inadequate as this may seem, if the goal is to find documents about data scientists then it might in fact work well enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48902"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distinct set of our words\n",
    "word_list = list(set([word for obs in bow_list for word in obs[0]]))\n",
    "len(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier with binary features\n",
    "\n",
    "One of the most simple types of machine learning models that is suitable here is the so called Naïve Bayes model. The simplest form of this model is of the binary classification type. There we assume that each observation consists of a vector of binary valued features $X=(x_1,x_2,…,x_k)$ (each component $x_j$ is either 0 or 1 valued) and a binary target value $Y$. By the basic Bayes theorem we have that\n",
    "\n",
    "\\begin{align*} p(Y \\mid x_1,x_2,...,x_k) &= \\frac{p(Y)\\ p(x_1,x_2,...,x_k \\mid Y)}{p(x_1,x_2,...,x_k)} \\\\\n",
    "&= \\frac{p(Y)\\ p(x_1,x_2,...,x_k \\mid Y)}{\\sum_{i=0}^1P(x_1,x_2,...,x_k\\mid Y = i)\\ p(Y=i)}\\end{align*}\n",
    "\n",
    "Here $X=(x_1,x_2,…,x_k)$ is a binary valued sequence of feature values for a given observation. In this case we 'only' need to estimate probabilities $P(Y=1)$, $P(Y=0)$, and $P(x_1,x_2,…,x_k|Y=0)$, $P(x_1,x_2,…,x_k|Y=1)$ for each possible binary sequence $x_1,x_2,...,x_k$ of feature values. Now for a feature vector of length $k$, there are $2^k$ distinct binary sequences of length $k$ (in each position we can have 0 or 1, so we indeed have to take k-th power of 2). So total number of parameters to estimate is $2\\cdot2^k +2$ = 'waaay too many!'.\n",
    "\n",
    "To get a feeling of this number, if we have many features, say as many as there are different words in a list of documents, of we would work with 200 different words, then $2^{200}=16^{50}$ which is a number larger then number of atoms in the universe! Also, Some of these combinations of binary values are typically very rare in our data which amounts to very low significance (certainty) of our estimate, and that translates directly to low predictive performance of our model.\n",
    "\n",
    "A way to mitigate a too high number of parameters to estimate is the so called Naïve Bayes assumption.  We thus assume that\n",
    "$$ P(x_1,x_2,...,x_k \\mid Y=0) = \\prod_{i=1}^{k}P(x_i \\mid Y=0)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Term-Document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 19min 34s, sys: 1.02 s, total: 1h 19min 35s\n",
      "Wall time: 1h 20min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "total_words = [words for doc in bow_list for words in doc[1]]\n",
    "\n",
    "n = 0\n",
    "col_ind = [None]*total_words\n",
    "row_ind = [None]*total_words\n",
    "data = [1]*total_words\n",
    "\n",
    "for row, doc in enumerate(bow_list):\n",
    "    for term in doc[0]:\n",
    "        col_ind[n] = word_list.index(term)\n",
    "        row_ind[n] = row\n",
    "\n",
    "size = (len(bow_list),len(word_list))\n",
    "indices = (row_ind, col_ind)\n",
    "word_occurence = csr_matrix((data, indices), size)\n",
    "target = np.array([row[1] for row in bow_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative approach by looping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving since really slow...\n",
    "# import scipy.sparse\n",
    "# scipy.sparse.save_npz('/home/dorian/workspace/data/tdm.npz', word_occurence)\n",
    "# word_occurence = scipy.sparse.load_npz('~/workspace/data/tdm.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate probabilities using TDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_y1 = word_occurence[target].sum(axis=0)/sum(target)\n",
    "prob_y0 = word_occurence[~target].sum(axis=0)/sum(~target)\n",
    "\n",
    "# How to calculate P(C|X) for prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.34627092e-01 0.00000000e+00 8.62314279e-06 ... 6.03619995e-05\n",
      "  0.00000000e+00 7.76082851e-05]]\n",
      "[[4.52236192e-01 4.59109721e-05 0.00000000e+00 ... 0.00000000e+00\n",
      "  4.59109721e-05 2.62348412e-05]]\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
